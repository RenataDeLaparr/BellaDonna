# Importing the libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, GlobalMaxPooling1D, Flatten
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt # graphs
----------------------------------------------------------------------------------------------
import os

current_directory = os.getcwd()
print("The current working directory:", current_directory)

os.chdir('C:\\BellaDonna')
print("The new working directory:", os.getcwd())
-----------------------------------------------------------------------------------------------
file_path = 'C:\\BellaDonna\\data\\data.json'  

with open(file_path, 'r', encoding='utf-8') as file:
    data = json.load(file)

df = pd.json_normalize(data, 'intents') 
print(df)
------------------------------------------------------------------------------------------------
tag_list = []
input_list = []  
responses = {}
for intent in data['intents']:  
    responses[intent['tag']] = intent['responses']
    
    for input_text in intent.get('input', []):
        input_list.append(input_text)  
        tag_list.append(intent['tag'])  
------------------------------------------------------------------------------------------------
#converting to dataframe
data = pd.DataFrame({
    "input": input_list,
    "tag": tag_list
})


print(type(data))
print(data.head()) #take a look at the couple first lines 
print(data.columns)

print(len(input_list))
print(len(tag_list))
-------------------------------------------------------------------------------------------------
#removing punctuations
import string
data['input'] = data['input'].apply(lambda wrd: ''.join([ltr.lower() for ltr in wrd if ltr not in string.punctuation]))
data['input']=data['input'].apply(lambda wrd: ' '.join(wrd))
data 
-------------------------------------------------------------------------------------------------
#printing the data
data
-------------------------------------------------------------------------------------------------
#tokenize the data
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=2000)
tokenizer.fit_on_texts(data['input'])
train = tokenizer.texts_to_sequences(data['input'])

# Padding
from tensorflow.keras.preprocessing.sequence import pad_sequences
x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(data['tag'])


print(data['input'])
print(train)
-----------------------------------------------------------------------------------------------
patterns_shape = x_train.shape[1]
print (patterns_shape)
-----------------------------------------------------------------------------------------------
#define vocabulary
vocabulary = len(tokenizer.word_index)
print("number of unique words : ",vocabulary)

output_length = len(le.classes_)
print("output length: ", output_length)
------------------------------------------------------------------------------------------------
input_shape = x_train.shape[1]  # A paddingolt szekvenciák hossza

# Creating the model
i = Input(shape=(input_shape,))
x = Embedding(vocabulary + 1, 10)(i)
x = LSTM(10, return_sequences=True)(x)
x = Flatten()(x) 
x = Dense(output_length, activation='softmax')(x)  # feltételezve, hogy 'output_length' a célváltozók egyedi értékeinek száma
model = Model(i, x)
------------------------------------------------------------------------------------------------
#compiling a model 
model.compile(loss="sparse_categorical_crossentropy", optimizer='adam',metrics=['accuracy'])
------------------------------------------------------------------------------------------------
#training the model
train = model.fit(x_train,y_train,epochs=500)
------------------------------------------------------------------------------------------------
#plotting model accuracy
plt.plot(train.history['accuracy'], label='training set accuracy')
plt.plot(train.history['loss'], label='training set loss')
plt.legend() 
------------------------------------------------------------------------------------------------
import random

# Main chatting loop
while True:
    texts_p = []
    prediction_input = input('You: ')  # Ask for user input

    # Normalize input by removing punctuation and converting to lowercase
    prediction_input_clean = ''.join([char.lower() for char in prediction_input if char not in string.punctuation])
    texts_p.append(prediction_input_clean)

    # First, check if the input matches any specific entities
    entity_info = find_entity_info('beilagen_saucen_extras', prediction_input_clean)
    if entity_info:
        # If a matching entity is found, use its details for the response
        print(f"BellaDonna: Name: {entity_info['name']}, Preis: {entity_info['price']}")
        continue  # Skip the rest of the loop and wait for the next user input

    # Tokenizing and padding if no direct entity match was found
    prediction_input = tokenizer.texts_to_sequences(texts_p)  
    prediction_input = np.array(prediction_input).reshape(-1)
    prediction_input = pad_sequences([prediction_input], maxlen=patterns_shape)

    # Getting output from model for the processed input
    output = model.predict(prediction_input)
    output = output.argmax()  # Find the index of the highest probability

    # Finding the right tag and predicting response based on the model output
    response_tag = le.inverse_transform([output])[0]  # Convert the output index back to a tag
    print("BellaDonna: ", random.choice(responses[response_tag]))  # Use the tag to find a random response

    # Check if it's time to end the conversation
    if response_tag == "goodbye":
        break  # Exit the loop if the response tag is "goodbye"




